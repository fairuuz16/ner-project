{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-31T10:04:10.967371Z",
     "iopub.status.busy": "2025-05-31T10:04:10.967072Z",
     "iopub.status.idle": "2025-05-31T10:04:13.715641Z",
     "shell.execute_reply": "2025-05-31T10:04:13.714262Z",
     "shell.execute_reply.started": "2025-05-31T10:04:10.967333Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 12:22:27.564898: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748866947.587391     205 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748866947.594185     205 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T10:06:55.516954Z",
     "iopub.status.busy": "2025-05-31T10:06:55.516629Z",
     "iopub.status.idle": "2025-05-31T10:06:55.610041Z",
     "shell.execute_reply": "2025-05-31T10:06:55.609214Z",
     "shell.execute_reply.started": "2025-05-31T10:06:55.516930Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/cleaned/cleaned_all_data.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>judul</th>\n",
       "      <th>tanggal</th>\n",
       "      <th>isi_berita</th>\n",
       "      <th>panjang_judul</th>\n",
       "      <th>panjang_isi_berita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.detik.com/sumbagsel/hukum-dan-krim...</td>\n",
       "      <td>4 Anak di Bawah Umur Tersangka Pembunuhan di P...</td>\n",
       "      <td>2024-09-05 20:30:00</td>\n",
       "      <td>Pelaku pembunuhan dan pemerkosaan AA (14) seor...</td>\n",
       "      <td>73</td>\n",
       "      <td>2370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.detik.com/jatim/hukum-dan-kriminal...</td>\n",
       "      <td>26 Tersangka Dibekuk Selama 3 Bulan Terakhir d...</td>\n",
       "      <td>2024-09-05 18:38:00</td>\n",
       "      <td>Dalam waktu kurang lebih 3 bulan, Polres Probo...</td>\n",
       "      <td>64</td>\n",
       "      <td>1764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.detik.com/jatim/hukum-dan-kriminal...</td>\n",
       "      <td>Kriminalitas Surabaya Marak, Pengamat Sebut Pe...</td>\n",
       "      <td>2024-09-05 02:01:00</td>\n",
       "      <td>Sejumlah kejadian kriminalitas kian marak terj...</td>\n",
       "      <td>72</td>\n",
       "      <td>1224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.detik.com/sumut/hukum-dan-kriminal...</td>\n",
       "      <td>Pekerja Kafe Disiram Air Keras hingga Wajah 'B...</td>\n",
       "      <td>2024-09-04 21:40:00</td>\n",
       "      <td>Seorang pekerja kafe di Cengkareng, MAS (32), ...</td>\n",
       "      <td>71</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.detik.com/sumut/hukum-dan-kriminal...</td>\n",
       "      <td>Hilang Nyawa Pria di Simalungun gegara Rebutan...</td>\n",
       "      <td>2024-09-03 09:03:00</td>\n",
       "      <td>Hanya gegara rebutan mikrofon untuk bernyanyi ...</td>\n",
       "      <td>70</td>\n",
       "      <td>1398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>https://www.detik.com/sumut/hukum-dan-kriminal...</td>\n",
       "      <td>Curi Mobil Ambulans Puskesmas, 2 Pria di Simal...</td>\n",
       "      <td>2024-09-07 19:00:00</td>\n",
       "      <td>Dua pria di Simalungun ditangkap personel kepo...</td>\n",
       "      <td>68</td>\n",
       "      <td>1778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>https://www.detik.com/sumbagsel/hukum-dan-krim...</td>\n",
       "      <td>Polisi Sebut Pelaku Utama Pembunuh Siswi SMP d...</td>\n",
       "      <td>2024-09-07 18:00:00</td>\n",
       "      <td>Empat pelaku pembunuhan dan pemerkosaan siswi ...</td>\n",
       "      <td>75</td>\n",
       "      <td>3003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>https://www.detik.com/sumut/hukum-dan-kriminal...</td>\n",
       "      <td>Pria di Batam Kepergok Lecehkan Tetangga-Curi ...</td>\n",
       "      <td>2024-09-07 14:29:00</td>\n",
       "      <td>Seorang pria berinisial KP (19) warga Tiban Ba...</td>\n",
       "      <td>59</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>https://www.detik.com/jabar/hukum-dan-kriminal...</td>\n",
       "      <td>Aksi 'Ninja' Curi 401 Kartu ATM Terekam CCTV, ...</td>\n",
       "      <td>2024-09-06 15:02:00</td>\n",
       "      <td>Sebanyak 401 lembar kartu ATM milik nasabah sa...</td>\n",
       "      <td>64</td>\n",
       "      <td>2347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>https://www.detik.com/sumbagsel/hukum-dan-krim...</td>\n",
       "      <td>Pembunuhan Siswi SMP di Kuburan Cina Palembang...</td>\n",
       "      <td>2024-09-06 07:00:00</td>\n",
       "      <td>Warga Palembang dibuat heboh dengan penetapan ...</td>\n",
       "      <td>77</td>\n",
       "      <td>1965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>636 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url  \\\n",
       "0    https://www.detik.com/sumbagsel/hukum-dan-krim...   \n",
       "1    https://www.detik.com/jatim/hukum-dan-kriminal...   \n",
       "2    https://www.detik.com/jatim/hukum-dan-kriminal...   \n",
       "3    https://www.detik.com/sumut/hukum-dan-kriminal...   \n",
       "4    https://www.detik.com/sumut/hukum-dan-kriminal...   \n",
       "..                                                 ...   \n",
       "631  https://www.detik.com/sumut/hukum-dan-kriminal...   \n",
       "632  https://www.detik.com/sumbagsel/hukum-dan-krim...   \n",
       "633  https://www.detik.com/sumut/hukum-dan-kriminal...   \n",
       "634  https://www.detik.com/jabar/hukum-dan-kriminal...   \n",
       "635  https://www.detik.com/sumbagsel/hukum-dan-krim...   \n",
       "\n",
       "                                                 judul              tanggal  \\\n",
       "0    4 Anak di Bawah Umur Tersangka Pembunuhan di P...  2024-09-05 20:30:00   \n",
       "1    26 Tersangka Dibekuk Selama 3 Bulan Terakhir d...  2024-09-05 18:38:00   \n",
       "2    Kriminalitas Surabaya Marak, Pengamat Sebut Pe...  2024-09-05 02:01:00   \n",
       "3    Pekerja Kafe Disiram Air Keras hingga Wajah 'B...  2024-09-04 21:40:00   \n",
       "4    Hilang Nyawa Pria di Simalungun gegara Rebutan...  2024-09-03 09:03:00   \n",
       "..                                                 ...                  ...   \n",
       "631  Curi Mobil Ambulans Puskesmas, 2 Pria di Simal...  2024-09-07 19:00:00   \n",
       "632  Polisi Sebut Pelaku Utama Pembunuh Siswi SMP d...  2024-09-07 18:00:00   \n",
       "633  Pria di Batam Kepergok Lecehkan Tetangga-Curi ...  2024-09-07 14:29:00   \n",
       "634  Aksi 'Ninja' Curi 401 Kartu ATM Terekam CCTV, ...  2024-09-06 15:02:00   \n",
       "635  Pembunuhan Siswi SMP di Kuburan Cina Palembang...  2024-09-06 07:00:00   \n",
       "\n",
       "                                            isi_berita  panjang_judul  \\\n",
       "0    Pelaku pembunuhan dan pemerkosaan AA (14) seor...             73   \n",
       "1    Dalam waktu kurang lebih 3 bulan, Polres Probo...             64   \n",
       "2    Sejumlah kejadian kriminalitas kian marak terj...             72   \n",
       "3    Seorang pekerja kafe di Cengkareng, MAS (32), ...             71   \n",
       "4    Hanya gegara rebutan mikrofon untuk bernyanyi ...             70   \n",
       "..                                                 ...            ...   \n",
       "631  Dua pria di Simalungun ditangkap personel kepo...             68   \n",
       "632  Empat pelaku pembunuhan dan pemerkosaan siswi ...             75   \n",
       "633  Seorang pria berinisial KP (19) warga Tiban Ba...             59   \n",
       "634  Sebanyak 401 lembar kartu ATM milik nasabah sa...             64   \n",
       "635  Warga Palembang dibuat heboh dengan penetapan ...             77   \n",
       "\n",
       "     panjang_isi_berita  \n",
       "0                  2370  \n",
       "1                  1764  \n",
       "2                  1224  \n",
       "3                  1995  \n",
       "4                  1398  \n",
       "..                  ...  \n",
       "631                1778  \n",
       "632                3003  \n",
       "633                2000  \n",
       "634                2347  \n",
       "635                1965  \n",
       "\n",
       "[636 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/cleaned/cleaned_all_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(636, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T10:06:40.408792Z",
     "iopub.status.busy": "2025-05-31T10:06:40.408461Z",
     "iopub.status.idle": "2025-05-31T10:06:52.890040Z",
     "shell.execute_reply": "2025-05-31T10:06:52.889070Z",
     "shell.execute_reply.started": "2025-05-31T10:06:40.408765Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"token-classification\", model=\"cahya/NusaBert-ner-v1.3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cahya/NusaBert-ner-v1.3\")\n",
    "model = AutoModel.from_pretrained(\"cahya/NusaBert-ner-v1.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dengan artikel pertama:\n",
      "Text: Pelaku pembunuhan dan pemerkosaan AA (14) seorang remaja putri di Kuburan Cina Palembang sudah ditangkap dan ditetapkan tersangka. Keempat tersangka merupakan anak di bawah umur, sama seperti korbanny...\n",
      "\n",
      "==================================================\n",
      "Raw NusaBert Output:\n",
      "  {'entity': 'B-PER', 'score': 0.9859017, 'index': 5, 'word': 'Ä AA', 'start': 33, 'end': 36}\n",
      "  {'entity': 'B-CRD', 'score': 0.99209154, 'index': 7, 'word': '14', 'start': 38, 'end': 40}\n",
      "  {'entity': 'B-LOC', 'score': 0.9789508, 'index': 13, 'word': 'Ä Kub', 'start': 65, 'end': 69}\n",
      "  {'entity': 'I-LOC', 'score': 0.9747813, 'index': 14, 'word': 'uran', 'start': 69, 'end': 73}\n",
      "  {'entity': 'I-LOC', 'score': 0.99543375, 'index': 15, 'word': 'Ä Cina', 'start': 73, 'end': 78}\n",
      "  {'entity': 'B-GPE', 'score': 0.59830356, 'index': 16, 'word': 'Ä Palembang', 'start': 78, 'end': 88}\n",
      "  {'entity': 'B-CRD', 'score': 0.5197823, 'index': 23, 'word': 'Ä Keempat', 'start': 130, 'end': 138}\n",
      "  {'entity': 'B-ORG', 'score': 0.9998621, 'index': 38, 'word': 'Ä Universitas', 'start': 222, 'end': 234}\n",
      "  {'entity': 'I-ORG', 'score': 0.9900771, 'index': 39, 'word': 'Ä Taman', 'start': 234, 'end': 240}\n",
      "  {'entity': 'I-ORG', 'score': 0.99865013, 'index': 40, 'word': 'Ä Siswa', 'start': 240, 'end': 246}\n",
      "  {'entity': 'I-ORG', 'score': 0.9793452, 'index': 41, 'word': 'Ä Palembang', 'start': 246, 'end': 256}\n",
      "  {'entity': 'B-PER', 'score': 0.99882406, 'index': 43, 'word': 'Ä Dr', 'start': 257, 'end': 260}\n",
      "  {'entity': 'I-PER', 'score': 0.969994, 'index': 44, 'word': 'Ä Azwar', 'start': 260, 'end': 266}\n",
      "  {'entity': 'I-PER', 'score': 0.97581494, 'index': 45, 'word': 'Ä Agus', 'start': 266, 'end': 271}\n",
      "  {'entity': 'B-CRD', 'score': 0.7957562, 'index': 99, 'word': 'Ä satu', 'start': 610, 'end': 615}\n",
      "  {'entity': 'B-PRD', 'score': 0.98397845, 'index': 103, 'word': 'Ä film', 'start': 640, 'end': 645}\n",
      "  {'entity': 'I-PRD', 'score': 0.9771921, 'index': 104, 'word': 'Ä biru', 'start': 645, 'end': 650}\n",
      "  {'entity': 'B-PRD', 'score': 0.9420712, 'index': 106, 'word': 'p', 'start': 652, 'end': 653}\n",
      "  {'entity': 'B-PRD', 'score': 0.94556487, 'index': 107, 'word': 'orno', 'start': 653, 'end': 657}\n",
      "  {'entity': 'B-DAT', 'score': 0.9985629, 'index': 111, 'word': 'Ä Kamis', 'start': 669, 'end': 675}\n",
      "  {'entity': 'I-DAT', 'score': 0.9769505, 'index': 112, 'word': 'Ä (', 'start': 675, 'end': 677}\n",
      "  {'entity': 'I-DAT', 'score': 0.98303556, 'index': 113, 'word': '5', 'start': 677, 'end': 678}\n",
      "  {'entity': 'I-DAT', 'score': 0.9833249, 'index': 114, 'word': '/', 'start': 678, 'end': 679}\n",
      "  {'entity': 'I-DAT', 'score': 0.9995976, 'index': 115, 'word': '9', 'start': 679, 'end': 680}\n",
      "  {'entity': 'I-DAT', 'score': 0.9901086, 'index': 116, 'word': '/', 'start': 680, 'end': 681}\n",
      "  {'entity': 'I-DAT', 'score': 0.99806243, 'index': 117, 'word': '202', 'start': 681, 'end': 684}\n",
      "  {'entity': 'I-DAT', 'score': 0.9797667, 'index': 118, 'word': '4', 'start': 684, 'end': 685}\n",
      "  {'entity': 'I-PRD', 'score': 0.7906883, 'index': 167, 'word': 'Ä porno', 'start': 948, 'end': 954}\n",
      "  {'entity': 'B-PRD', 'score': 0.9625985, 'index': 201, 'word': 'Ä handphone', 'start': 1158, 'end': 1168}\n",
      "  {'entity': 'B-PRD', 'score': 0.71668386, 'index': 263, 'word': 'Ä medsos', 'start': 1547, 'end': 1554}\n",
      "  {'entity': 'B-PER', 'score': 0.9968021, 'index': 276, 'word': 'Ä Azwar', 'start': 1620, 'end': 1626}\n",
      "\n",
      "Unique entity labels found:\n",
      "  B-CRD -> CRD\n",
      "  B-DAT -> DAT\n",
      "  B-GPE -> GPE\n",
      "  B-LOC -> LOC\n",
      "  B-ORG -> ORG\n",
      "  B-PER -> PER\n",
      "  B-PRD -> PRD\n",
      "  I-DAT -> DAT\n",
      "  I-LOC -> LOC\n",
      "  I-ORG -> ORG\n",
      "  I-PER -> PER\n",
      "  I-PRD -> PRD\n"
     ]
    }
   ],
   "source": [
    "def debug_nusabert_output(text, pipe):\n",
    "    try:\n",
    "        raw_entities = pipe(text)\n",
    "        print(\"Raw NusaBert Output:\")\n",
    "        for entity in raw_entities:\n",
    "            print(f\"  {entity}\")\n",
    "        \n",
    "        print(\"\\nUnique entity labels found:\")\n",
    "        unique_labels = set()\n",
    "        for entity in raw_entities:\n",
    "            original_label = entity['entity']\n",
    "            cleaned_label = original_label.replace('B-', '').replace('I-', '')\n",
    "            unique_labels.add(f\"{original_label} -> {cleaned_label}\")\n",
    "        \n",
    "        for label in sorted(unique_labels):\n",
    "            print(f\"  {label}\")\n",
    "            \n",
    "        return raw_entities\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test dengan satu artikel\n",
    "test_text = df.iloc[0]['isi_berita']\n",
    "print(\"Testing dengan artikel pertama:\")\n",
    "print(f\"Text: {test_text[:200]}...\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "debug_output = debug_nusabert_output(test_text, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T10:39:22.334598Z",
     "iopub.status.busy": "2025-05-31T10:39:22.333940Z",
     "iopub.status.idle": "2025-05-31T10:51:58.169098Z",
     "shell.execute_reply": "2025-05-31T10:51:58.167891Z",
     "shell.execute_reply.started": "2025-05-31T10:39:22.334569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CRIME_TYPES = [\n",
    "    'pembunuhan', 'pemerkosaan', 'penganiayaan', 'pembacokan', 'penikaman',\n",
    "    'penyiraman air keras', 'penembakan', 'pengeroyokan', 'mutilasi',\n",
    "    'pencurian', 'curanmor', 'pencurian motor', 'pencurian dengan kekerasan',\n",
    "    'perampokan', 'begal', 'penjambretan', 'penggelapan',\n",
    "    'pencurian spesialis bongkar warung', 'bajing loncat', 'pembobolan',\n",
    "    'kasus sabu', 'peredaran obat-obatan terlarang', 'kasus narkoba',\n",
    "    'penyalahgunaan narkoba', 'peredaran pil koplo', 'pencabulan', 'sodomi',\n",
    "    'pelecehan seksual', 'korupsi dana desa', 'korupsi', 'uang palsu',\n",
    "    'tawuran', 'aksi perusakan', 'pemalsuan dokumen', 'penjualan miras ilegal',\n",
    "    'perjudian', 'sabung ayam', 'penculikan', 'perdagangan orang',\n",
    "    'pembakaran', 'penipuan', 'penyelundupan', 'pemalakan', 'gangster',\n",
    "    'perampokan agen ATM mini', 'terorisme', 'radikalisme', 'pencucian uang',\n",
    "    'pengeroyokan', 'perkelahian', 'pembegalan', 'penodongan'\n",
    "]\n",
    "\n",
    "EVIDENCE_TYPES = [\n",
    "    'barang bukti', 'bukti', 'alat bukti', 'saksi mata', 'rekaman cctv',\n",
    "    'rekaman video', 'foto', 'dokumen', 'surat', 'kontrak', 'kwitansi',\n",
    "    'nota', 'struk', 'invoice', 'rekening bank', 'transfer', 'sms',\n",
    "    'percakapan', 'chat', 'whatsapp', 'telegram', 'email', 'surat elektronik',\n",
    "    'keterangan saksi', 'pengakuan', 'visum', 'autopsi', 'hasil lab',\n",
    "    'tes dna', 'tes narkoba', 'sidik jari', 'jejak', 'darah', 'rambut',\n",
    "    'dna', 'sertipikat', 'ijazah', 'kartu identitas', 'sim', 'stnk',\n",
    "    'bpkb', 'akta', 'surat nikah', 'hard disk', 'laptop', 'handphone',\n",
    "    'flashdisk', 'cd', 'dvd', 'kartu memori', 'memory card', 'cctv',\n",
    "    'kamera', 'perekam', 'alat perekam', 'microphone', 'handycam'\n",
    "]\n",
    "\n",
    "def detect_crime_types(text):\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    detected_crimes = []\n",
    "    \n",
    "    for crime in CRIME_TYPES:\n",
    "        # Exact match pattern\n",
    "        pattern = r'\\b' + re.escape(crime.lower()) + r'\\b'\n",
    "        matches = re.finditer(pattern, text_lower)\n",
    "        \n",
    "        for match in matches:\n",
    "            detected_crimes.append({\n",
    "                'entity': 'CRIMETYPE',\n",
    "                'word': crime,\n",
    "                'start': match.start(),\n",
    "                'end': match.end(),\n",
    "                'score': 1.0\n",
    "            })\n",
    "    \n",
    "    # Additional pattern-based crime detection\n",
    "    crime_patterns = {\n",
    "        'pembunuhan': [r'\\b(membunuh|dibunuh|terbunuh|bunuh|tewas.*dibunuh)\\b'],\n",
    "        'pencurian': [r'\\b(mencuri|dicuri|tercuri|curi|mengambil.*tanpa.*izin)\\b'],\n",
    "        'narkoba': [r'\\b(narkoba|narkotika|psikotropika|sabu|ganja|ekstasi|kokain|heroin|shabu)\\b'],\n",
    "        'korupsi': [r'\\b(korupsi|suap|gratifikasi|penyalahgunaan.*jabatan|mark.*up)\\b'],\n",
    "        'penipuan': [r'\\b(menipu|ditipu|tertipu|penipuan|penipu|modus)\\b'],\n",
    "        'pemerkosaan': [r'\\b(perkosa|memperkosa|diperkosa|cabul|mencabuli|dicabuli)\\b']\n",
    "    }\n",
    "    \n",
    "    for crime_type, patterns in crime_patterns.items():\n",
    "        for pattern in patterns:\n",
    "            matches = re.finditer(pattern, text_lower)\n",
    "            for match in matches:\n",
    "                # Check if not already detected\n",
    "                is_duplicate = any(\n",
    "                    abs(match.start() - existing['start']) < 5 \n",
    "                    for existing in detected_crimes\n",
    "                )\n",
    "                if not is_duplicate:\n",
    "                    detected_crimes.append({\n",
    "                        'entity': 'CRIMETYPE',\n",
    "                        'word': crime_type,\n",
    "                        'start': match.start(),\n",
    "                        'end': match.end(),\n",
    "                        'score': 0.9\n",
    "                    })\n",
    "    \n",
    "    return detected_crimes\n",
    "\n",
    "def detect_evidence_types(text):\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    detected_evidence = []\n",
    "    \n",
    "    for evidence in EVIDENCE_TYPES:\n",
    "        pattern = r'\\b' + re.escape(evidence.lower()) + r'\\b'\n",
    "        matches = re.finditer(pattern, text_lower)\n",
    "        \n",
    "        for match in matches:\n",
    "            detected_evidence.append({\n",
    "                'entity': 'EVIDENCE',\n",
    "                'word': evidence,\n",
    "                'start': match.start(),\n",
    "                'end': match.end(),\n",
    "                'score': 1.0\n",
    "            })\n",
    "    \n",
    "    # Additional evidence patterns\n",
    "    evidence_patterns = {\n",
    "        'dokumen_resmi': [r'\\b(surat.*resmi|dokumen.*resmi|akta.*resmi|sertipikat.*resmi)\\b'],\n",
    "        'rekaman_digital': [r'\\b(rekaman.*digital|file.*digital|data.*digital)\\b'],\n",
    "        'keterangan_saksi': [r'\\b(keterangan.*saksi|kesaksian|testimoni)\\b'],\n",
    "        'hasil_forensik': [r'\\b(hasil.*forensik|analisis.*forensik|lab.*forensik)\\b']\n",
    "    }\n",
    "    \n",
    "    for evidence_type, patterns in evidence_patterns.items():\n",
    "        for pattern in patterns:\n",
    "            matches = re.finditer(pattern, text_lower)\n",
    "            for match in matches:\n",
    "                is_duplicate = any(\n",
    "                    abs(match.start() - existing['start']) < 5 \n",
    "                    for existing in detected_evidence\n",
    "                )\n",
    "                if not is_duplicate:\n",
    "                    detected_evidence.append({\n",
    "                        'entity': 'EVIDENCE',\n",
    "                        'word': evidence_type.replace('_', ' '),\n",
    "                        'start': match.start(),\n",
    "                        'end': match.end(),\n",
    "                        'score': 0.9\n",
    "                    })\n",
    "    \n",
    "    return detected_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dates_pattern(text):\n",
    "\n",
    "    import re\n",
    "    detected_dates = []\n",
    "    \n",
    "    # Pattern untuk berbagai format tanggal Indonesia\n",
    "    date_patterns = [\n",
    "        r'\\b\\d{1,2}[\\s\\-/]\\w+[\\s\\-/]\\d{4}\\b',  # 15 Januari 2024, 15-01-2024\n",
    "        r'\\b\\d{1,2}[\\s\\-/]\\d{1,2}[\\s\\-/]\\d{4}\\b',  # 15/01/2024, 15-01-2024\n",
    "        r'\\b\\d{4}[\\s\\-/]\\d{1,2}[\\s\\-/]\\d{1,2}\\b',  # 2024-01-15\n",
    "        r'\\b(?:senin|selasa|rabu|kamis|jumat|sabtu|minggu)\\b',  # Hari\n",
    "        r'\\b(?:januari|februari|maret|april|mei|juni|juli|agustus|september|oktober|november|desember)\\b',  # Bulan\n",
    "        r'\\b\\d{1,2}\\s*(?:januari|februari|maret|april|mei|juni|juli|agustus|september|oktober|november|desember)\\s*\\d{4}\\b',  # 15 Januari 2024\n",
    "        r'\\bpukul\\s*\\d{1,2}[:.]\\d{2}\\b',  # pukul 14:30\n",
    "        r'\\b\\d{1,2}[:.]\\d{2}\\s*(?:WIB|WITA|WIT)\\b',  # 14:30 WIB\n",
    "        r'\\b(?:kemarin|hari\\s*ini|besok|lusa)\\b',  # Referensi waktu relatif\n",
    "        r'\\b(?:pagi|siang|sore|malam)\\s*(?:hari|ini|kemarin|tadi)?\\b'  # Waktu dalam hari\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for pattern in date_patterns:\n",
    "        matches = re.finditer(pattern, text_lower, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            detected_dates.append({\n",
    "                'entity': 'DAT',\n",
    "                'word': text[match.start():match.end()],\n",
    "                'start': match.start(),\n",
    "                'end': match.end(),\n",
    "                'score': 0.95\n",
    "            })\n",
    "    \n",
    "    return detected_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_nusabert_enhanced(text, pipe):\n",
    "    \n",
    "    try:\n",
    "        entities = pipe(text)\n",
    "        \n",
    "        # Enhanced entity mapping\n",
    "        entity_mapping = {\n",
    "            'PER': 'PER',\n",
    "            'PERSON': 'PER', \n",
    "            'B-PER': 'PER',\n",
    "            'I-PER': 'PER',\n",
    "            \n",
    "            'LOC': 'LOC',\n",
    "            'LOCATION': 'LOC',\n",
    "            'B-LOC': 'LOC',\n",
    "            'I-LOC': 'LOC',\n",
    "            'GPE': 'GPE',  \n",
    "            'B-GPE': 'GPE',\n",
    "            'I-GPE': 'GPE',\n",
    "            \n",
    "            'ORG': 'NOR',  \n",
    "            'ORGANIZATION': 'NOR',\n",
    "            'B-ORG': 'NOR',\n",
    "            'I-ORG': 'NOR',\n",
    "            \n",
    "            'DATE': 'DAT',\n",
    "            'TIME': 'DAT', \n",
    "            'B-DATE': 'DAT',\n",
    "            'I-DATE': 'DAT',\n",
    "            'B-TIME': 'DAT',\n",
    "            'I-TIME': 'DAT',\n",
    "            \n",
    "            'LAW': 'LAW',  \n",
    "            'LEGAL': 'LAW',\n",
    "            'B-LAW': 'LAW',\n",
    "            'I-LAW': 'LAW',\n",
    "            'MISC': 'LAW'  \n",
    "        }\n",
    "        \n",
    "        filtered_entities = []\n",
    "        \n",
    "        for entity in entities:\n",
    "            original_label = entity['entity']\n",
    "            # Remove B- and I- prefixes\n",
    "            clean_label = original_label.replace('B-', '').replace('I-', '')\n",
    "            \n",
    "            # Map to target entities\n",
    "            mapped_label = entity_mapping.get(clean_label, entity_mapping.get(original_label, None))\n",
    "            \n",
    "            if mapped_label:\n",
    "                filtered_entities.append({\n",
    "                    'entity': mapped_label,\n",
    "                    'word': entity['word'],\n",
    "                    'start': entity['start'],\n",
    "                    'end': entity['end'],\n",
    "                    'score': float(entity['score'])\n",
    "                })\n",
    "        \n",
    "        return filtered_entities\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error dalam NER: {e}\")\n",
    "        return []\n",
    "\n",
    "def merge_consecutive_entities(entities):\n",
    "    if not entities:\n",
    "        return entities\n",
    "    \n",
    "    entities.sort(key=lambda x: x['start'])\n",
    "    \n",
    "    merged = []\n",
    "    current_entity = entities[0].copy()\n",
    "    \n",
    "    for i in range(1, len(entities)):\n",
    "        next_entity = entities[i]\n",
    "        \n",
    "        if (current_entity['entity'] == next_entity['entity'] and \n",
    "            current_entity['end'] >= next_entity['start'] - 3): \n",
    "            \n",
    "            if current_entity['word'].endswith('##') or next_entity['word'].startswith('##'):\n",
    "                current_word = current_entity['word'].replace('##', '')\n",
    "                next_word = next_entity['word'].replace('##', '')\n",
    "                current_entity['word'] = current_word + next_word\n",
    "            else:\n",
    "                current_entity['word'] += ' ' + next_entity['word']\n",
    "            \n",
    "            current_entity['end'] = next_entity['end']\n",
    "            current_entity['score'] = float((current_entity['score'] + next_entity['score']) / 2)\n",
    "        else:\n",
    "            merged.append(current_entity)\n",
    "            current_entity = next_entity.copy()\n",
    "    \n",
    "    merged.append(current_entity)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DATE detection standalone:\n",
      "Found 7 dates:\n",
      "  - 15 Januari 2024 (score: 0.95)\n",
      "  - Januari (score: 0.95)\n",
      "  - 15 Januari 2024 (score: 0.95)\n",
      "  - pukul 14:30 (score: 0.95)\n",
      "  - 14:30 WIB (score: 0.95)\n",
      "  - kemarin (score: 0.95)\n",
      "  - sore (score: 0.95)\n",
      "\n",
      "Testing full extraction:\n",
      "DAT entities in full extraction: 4\n",
      "  - 15 Januari 2024 (score: 0.95)\n",
      "  - pukul 14:30 (score: 0.95)\n",
      "  - kemarin (score: 0.95)\n",
      "  - sore (score: 0.95)\n"
     ]
    }
   ],
   "source": [
    "def extract_all_entities_enhanced(text, pipe):\n",
    "    nusabert_entities = extract_entities_nusabert_enhanced(text, pipe)\n",
    "    \n",
    "    crime_entities = detect_crime_types(text)\n",
    "    evidence_entities = detect_evidence_types(text)\n",
    "    date_entities = detect_dates_pattern(text)  # Tambah deteksi DATE\n",
    "    \n",
    "    all_entities = nusabert_entities + crime_entities + evidence_entities + date_entities\n",
    "    \n",
    "    all_entities.sort(key=lambda x: x['start'])\n",
    "    \n",
    "    filtered_entities = []\n",
    "    for entity in all_entities:\n",
    "        is_overlap = False\n",
    "        for existing in filtered_entities:\n",
    "            if (entity['start'] < existing['end'] and entity['end'] > existing['start']):\n",
    "                if entity['score'] <= existing['score']:\n",
    "                    is_overlap = True\n",
    "                    break\n",
    "        \n",
    "        if not is_overlap:\n",
    "            filtered_entities = [e for e in filtered_entities \n",
    "                               if not (entity['start'] < e['end'] and entity['end'] > e['start'] and entity['score'] > e['score'])]\n",
    "            filtered_entities.append(entity)\n",
    "    \n",
    "    entity_groups = defaultdict(list)\n",
    "    for entity in filtered_entities:\n",
    "        entity_groups[entity['entity']].append(entity)\n",
    "    \n",
    "    merged_entities = []\n",
    "    for entity_type, entities in entity_groups.items():\n",
    "        if entity_type in ['CRIMETYPE', 'EVIDENCE', 'DAT']:  \n",
    "            merged_entities.extend(entities)\n",
    "        else:\n",
    "            merged = merge_consecutive_entities(entities)\n",
    "            merged_entities.extend(merged)\n",
    "    \n",
    "    # Final sort by position\n",
    "    merged_entities.sort(key=lambda x: x['start'])\n",
    "    \n",
    "    return merged_entities\n",
    "\n",
    "print(\"Testing DATE detection standalone:\")\n",
    "test_text_with_dates = \"Kejadian ini terjadi pada tanggal 15 Januari 2024 pukul 14:30 WIB di Jakarta kemarin sore.\"\n",
    "date_entities = detect_dates_pattern(test_text_with_dates)\n",
    "print(f\"Found {len(date_entities)} dates:\")\n",
    "for entity in date_entities:\n",
    "    print(f\"  - {entity['word']} (score: {entity['score']})\")\n",
    "\n",
    "print(\"\\nTesting full extraction:\")\n",
    "all_entities = extract_all_entities_enhanced(test_text_with_dates, pipe)\n",
    "dat_entities = [e for e in all_entities if e['entity'] == 'DAT']\n",
    "print(f\"DAT entities in full extraction: {len(dat_entities)}\")\n",
    "for entity in dat_entities:\n",
    "    print(f\"  - {entity['word']} (score: {entity['score']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe_entities_enhanced(df, pipe, sample_size=None):\n",
    "    if sample_size:\n",
    "        df_sample = df.head(sample_size)\n",
    "    else:\n",
    "        df_sample = df\n",
    "    \n",
    "    results = []\n",
    "    target_entities = ['GPE', 'LOC', 'NOR', 'LAW', 'DAT', 'PER', 'CRIMETYPE', 'EVIDENCE']\n",
    "    \n",
    "    for idx, row in df_sample.iterrows():\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processing artikel {idx+1}/{len(df_sample)}\")\n",
    "        \n",
    "        try:\n",
    "            entities = extract_all_entities_enhanced(row['isi_berita'], pipe)\n",
    "            \n",
    "            # Group entities by type\n",
    "            entity_dict = defaultdict(list)\n",
    "            for entity in entities:\n",
    "                if entity['entity'] in target_entities:\n",
    "                    entity_dict[entity['entity']].append({\n",
    "                        'word': entity['word'],\n",
    "                        'start': int(entity['start']),\n",
    "                        'end': int(entity['end']),\n",
    "                        'score': float(entity['score'])\n",
    "                    })\n",
    "            \n",
    "            result = {\n",
    "                'index': int(idx),\n",
    "                'url': row['url'],\n",
    "                'judul': row['judul'],\n",
    "                'tanggal': row['tanggal'],\n",
    "                'entities': dict(entity_dict),\n",
    "                'total_entities': len(entities)\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_enhanced_results(results, base_filename='enhanced_entity_results'):\n",
    "    \n",
    "    flattened_data = []\n",
    "    target_entities = ['GPE', 'LOC', 'NOR', 'LAW', 'DAT', 'PER', 'CRIMETYPE', 'EVIDENCE']\n",
    "    \n",
    "    for result in results:\n",
    "        base_row = {\n",
    "            'index': result['index'],\n",
    "            'url': result['url'],\n",
    "            'judul': result['judul'],\n",
    "            'tanggal': result['tanggal'],\n",
    "            'total_entities': result['total_entities']\n",
    "        }\n",
    "        \n",
    "        for entity_type in target_entities:\n",
    "            entities = result['entities'].get(entity_type, [])\n",
    "            base_row[f'{entity_type.lower()}_count'] = len(entities)\n",
    "            base_row[f'{entity_type.lower()}_entities'] = '; '.join([e['word'] for e in entities])\n",
    "            \n",
    "            if entities:\n",
    "                scores = [e['score'] for e in entities]\n",
    "                base_row[f'{entity_type.lower()}_avg_score'] = round(sum(scores) / len(scores), 3)\n",
    "                base_row[f'{entity_type.lower()}_max_score'] = round(max(scores), 3)\n",
    "            else:\n",
    "                base_row[f'{entity_type.lower()}_avg_score'] = 0\n",
    "                base_row[f'{entity_type.lower()}_max_score'] = 0\n",
    "        \n",
    "        flattened_data.append(base_row)\n",
    "    \n",
    "    # Save CSV\n",
    "    df_results = pd.DataFrame(flattened_data)\n",
    "    df_results.to_csv(f'{base_filename}.csv', index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Enhanced results saved to {base_filename}.csv\")\n",
    "    print(f\"Total articles processed: {len(results)}\")\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing enhanced entity extraction dengan sample...\n",
      "Test text preview: Pelaku pembunuhan dan pemerkosaan AA (14) seorang remaja putri di Kuburan Cina Palembang sudah ditangkap dan ditetapkan tersangka. Keempat tersangka merupakan anak di bawah umur, sama seperti korbannya. Pengamat Hukum dari Universitas Taman Siswa Palembang, Dr Azwar Agus menyampaikan turut prihatin ...\n",
      "\n",
      "==================================================\n",
      "\n",
      "Total entities found: 12\n",
      "\n",
      "Entity summary:\n",
      "  CRIMETYPE: 3 entities\n",
      "    Examples: pembunuhan, pemerkosaan, pembunuhan\n",
      "  PER: 3 entities\n",
      "    Examples: Ä AA, Ä Dr Ä Azwar Ä Agus, Ä Azwar\n",
      "  LOC: 1 entities\n",
      "    Examples: Ä Kub uran Ä Cina\n",
      "  GPE: 1 entities\n",
      "    Examples: Ä Palembang\n",
      "  NOR: 1 entities\n",
      "    Examples: Ä Universitas Ä Taman Ä Siswa Ä Palembang\n",
      "  DAT: 2 entities\n",
      "    Examples: Kamis, 5/9/2024\n",
      "  EVIDENCE: 1 entities\n",
      "    Examples: handphone\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing enhanced entity extraction dengan sample...\")\n",
    "test_text = df.iloc[0]['isi_berita']\n",
    "print(f\"Test text preview: {test_text[:300]}...\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "test_entities = extract_all_entities_enhanced(test_text, pipe)\n",
    "print(f\"\\nTotal entities found: {len(test_entities)}\")\n",
    "\n",
    "# Group by entity type\n",
    "entity_summary = defaultdict(list)\n",
    "for entity in test_entities:\n",
    "    entity_summary[entity['entity']].append(entity['word'])\n",
    "\n",
    "print(\"\\nEntity summary:\")\n",
    "for entity_type, words in entity_summary.items():\n",
    "    print(f\"  {entity_type}: {len(words)} entities\")\n",
    "    print(f\"    Examples: {', '.join(words[:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample data (first 100 articles)...\n",
      "Processing artikel 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced results saved to sample_enhanced_results.csv\n",
      "Total articles processed: 100\n",
      "\n",
      "Sample results preview:\n",
      "   index                                                url  \\\n",
      "0      0  https://www.detik.com/sumbagsel/hukum-dan-krim...   \n",
      "1      1  https://www.detik.com/jatim/hukum-dan-kriminal...   \n",
      "2      2  https://www.detik.com/jatim/hukum-dan-kriminal...   \n",
      "3      3  https://www.detik.com/sumut/hukum-dan-kriminal...   \n",
      "4      4  https://www.detik.com/sumut/hukum-dan-kriminal...   \n",
      "\n",
      "                                               judul              tanggal  \\\n",
      "0  4 Anak di Bawah Umur Tersangka Pembunuhan di P...  2024-09-05 20:30:00   \n",
      "1  26 Tersangka Dibekuk Selama 3 Bulan Terakhir d...  2024-09-05 18:38:00   \n",
      "2  Kriminalitas Surabaya Marak, Pengamat Sebut Pe...  2024-09-05 02:01:00   \n",
      "3  Pekerja Kafe Disiram Air Keras hingga Wajah 'B...  2024-09-04 21:40:00   \n",
      "4  Hilang Nyawa Pria di Simalungun gegara Rebutan...  2024-09-03 09:03:00   \n",
      "\n",
      "   total_entities  gpe_count  \\\n",
      "0              12          1   \n",
      "1              26          1   \n",
      "2              14          2   \n",
      "3              16          2   \n",
      "4              15          2   \n",
      "\n",
      "                                        gpe_entities  gpe_avg_score  \\\n",
      "0                                         Ä Palembang          0.598   \n",
      "1                                 Ä Kota Ä Probolinggo          0.941   \n",
      "2                               Ä Surabaya; Ä Surabaya          1.000   \n",
      "3           Ä Cengkareng; Ä Cengkareng Ä Jakarta Ä Barat          0.990   \n",
      "4  iran; or Ä Kecamatan Ä Pan ei Ä Kabupaten Ä Simalu...          0.734   \n",
      "\n",
      "   gpe_max_score  loc_count  ... per_avg_score  per_max_score  \\\n",
      "0          0.598          1  ...         0.988          0.997   \n",
      "1          0.941          0  ...         0.981          0.987   \n",
      "2          1.000          0  ...         0.812          0.955   \n",
      "3          0.995          2  ...         0.952          0.999   \n",
      "4          0.948          2  ...         0.868          0.980   \n",
      "\n",
      "   crimetype_count                                 crimetype_entities  \\\n",
      "0                3                pembunuhan; pemerkosaan; pembunuhan   \n",
      "1               10  pembunuhan; kasus sabu; pembunuhan; penggelapa...   \n",
      "2                2                                    gangster; begal   \n",
      "3                1                               penyiraman air keras   \n",
      "4                0                                                      \n",
      "\n",
      "  crimetype_avg_score  crimetype_max_score  evidence_count  \\\n",
      "0                1.00                  1.0               1   \n",
      "1                0.99                  1.0               2   \n",
      "2                1.00                  1.0               0   \n",
      "3                1.00                  1.0               0   \n",
      "4                0.00                  0.0               0   \n",
      "\n",
      "       evidence_entities evidence_avg_score  evidence_max_score  \n",
      "0              handphone                1.0                 1.0  \n",
      "1  barang bukti; dokumen                1.0                 1.0  \n",
      "2                                       0.0                 0.0  \n",
      "3                                       0.0                 0.0  \n",
      "4                                       0.0                 0.0  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing sample data (first 100 articles)...\")\n",
    "sample_results = process_dataframe_entities_enhanced(df, pipe, sample_size=100)\n",
    "sample_output = save_enhanced_results(sample_results, 'sample_enhanced_results')\n",
    "\n",
    "print(\"\\nSample results preview:\")\n",
    "print(sample_output.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all articles...\n",
      "Processing artikel 1/636\n",
      "Processing artikel 101/636\n",
      "Processing artikel 201/636\n",
      "Processing artikel 301/636\n",
      "Processing artikel 401/636\n",
      "Processing artikel 501/636\n",
      "Processing artikel 601/636\n",
      "Enhanced results saved to all_enhanced_entity_results.csv\n",
      "Total articles processed: 636\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing all articles...\")\n",
    "all_results = process_dataframe_entities_enhanced(df, pipe)\n",
    "enhanced_output = save_enhanced_results(all_results, 'all_enhanced_entity_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENHANCED ENTITY STATISTICS ===\n",
      "\n",
      "GPE:\n",
      "  Total entities: 2396\n",
      "  Coverage: 96.9% (616/636 articles)\n",
      "  Average per article: 3.77\n",
      "  Average confidence: 0.912\n",
      "\n",
      "LOC:\n",
      "  Total entities: 1662\n",
      "  Coverage: 84.9% (540/636 articles)\n",
      "  Average per article: 2.61\n",
      "  Average confidence: 0.703\n",
      "\n",
      "NOR:\n",
      "  Total entities: 861\n",
      "  Coverage: 55.5% (353/636 articles)\n",
      "  Average per article: 1.35\n",
      "  Average confidence: 0.440\n",
      "\n",
      "LAW:\n",
      "  Total entities: 452\n",
      "  Coverage: 31.6% (201/636 articles)\n",
      "  Average per article: 0.71\n",
      "  Average confidence: 0.266\n",
      "\n",
      "DAT:\n",
      "  Total entities: 3495\n",
      "  Coverage: 99.5% (633/636 articles)\n",
      "  Average per article: 5.50\n",
      "  Average confidence: 0.946\n",
      "\n",
      "PER:\n",
      "  Total entities: 7270\n",
      "  Coverage: 99.7% (634/636 articles)\n",
      "  Average per article: 11.43\n",
      "  Average confidence: 0.918\n",
      "\n",
      "CRIMETYPE:\n",
      "  Total entities: 2556\n",
      "  Coverage: 85.2% (542/636 articles)\n",
      "  Average per article: 4.02\n",
      "  Average confidence: 0.827\n",
      "\n",
      "EVIDENCE:\n",
      "  Total entities: 927\n",
      "  Coverage: 58.6% (373/636 articles)\n",
      "  Average per article: 1.46\n",
      "  Average confidence: 0.586\n",
      "\n",
      "=== SUMMARY TABLE ===\n",
      "Entity_Type  Total_Entities  Articles_with_Entity  Coverage_Percentage  Avg_per_Article  Avg_Confidence_Score  Max_in_Single_Article\n",
      "        GPE            2396                   616                 96.9             3.77                 0.912                     32\n",
      "        LOC            1662                   540                 84.9             2.61                 0.703                     18\n",
      "        NOR             861                   353                 55.5             1.35                 0.440                     14\n",
      "        LAW             452                   201                 31.6             0.71                 0.266                     16\n",
      "        DAT            3495                   633                 99.5             5.50                 0.946                     33\n",
      "        PER            7270                   634                 99.7            11.43                 0.918                     86\n",
      "  CRIMETYPE            2556                   542                 85.2             4.02                 0.827                     27\n",
      "   EVIDENCE             927                   373                 58.6             1.46                 0.586                     16\n"
     ]
    }
   ],
   "source": [
    "def analyze_enhanced_entity_statistics(output_df):\n",
    "\n",
    "    print(\"=== ENHANCED ENTITY STATISTICS ===\\n\")\n",
    "    \n",
    "    target_entities = ['gpe', 'loc', 'nor', 'law', 'dat', 'per', 'crimetype', 'evidence']\n",
    "    total_articles = len(output_df)\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for entity_type in target_entities:\n",
    "        count_col = f'{entity_type}_count'\n",
    "        entities_col = f'{entity_type}_entities'\n",
    "        score_col = f'{entity_type}_avg_score'\n",
    "        \n",
    "        if count_col in output_df.columns:\n",
    "            total_entities = output_df[count_col].sum()\n",
    "            articles_with_entity = len(output_df[output_df[count_col] > 0])\n",
    "            coverage = (articles_with_entity / total_articles) * 100\n",
    "            avg_per_article = total_entities / total_articles\n",
    "            avg_score = output_df[score_col].mean() if score_col in output_df.columns else 0\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Entity_Type': entity_type.upper(),\n",
    "                'Total_Entities': int(total_entities),\n",
    "                'Articles_with_Entity': articles_with_entity,\n",
    "                'Coverage_Percentage': round(coverage, 1),\n",
    "                'Avg_per_Article': round(avg_per_article, 2),\n",
    "                'Avg_Confidence_Score': round(avg_score, 3),\n",
    "                'Max_in_Single_Article': int(output_df[count_col].max())\n",
    "            })\n",
    "            \n",
    "            print(f\"{entity_type.upper()}:\")\n",
    "            print(f\"  Total entities: {total_entities}\")\n",
    "            print(f\"  Coverage: {coverage:.1f}% ({articles_with_entity}/{total_articles} articles)\")\n",
    "            print(f\"  Average per article: {avg_per_article:.2f}\")\n",
    "            print(f\"  Average confidence: {avg_score:.3f}\")\n",
    "            print()\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"=== SUMMARY TABLE ===\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Analyze enhanced results\n",
    "if 'enhanced_output' in locals():\n",
    "    enhanced_stats = analyze_enhanced_entity_statistics(enhanced_output)\n",
    "else:\n",
    "    enhanced_stats = analyze_enhanced_entity_statistics(sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DETAILED ENHANCED ENTITY ANALYSIS ===\n",
      "\n",
      "GPE Analysis:\n",
      "  Total unique gpes: 1320\n",
      "  Total gpe mentions: 2396\n",
      "  Most common gpes (top 10):\n",
      "    Ä Surabaya: 55 times\n",
      "    Ä Bali: 45 times\n",
      "    Ä Kota Ä Medan: 41 times\n",
      "    Ä Sumut: 32 times\n",
      "    Ä Desa: 25 times\n",
      "    Ä Kota Ä Medan Ä Sumatera Ä Utara Sumut: 24 times\n",
      "    Ä Rusia: 20 times\n",
      "    Ä Malaysia: 20 times\n",
      "    Ä Garut: 17 times\n",
      "    Ä Aceh: 17 times\n",
      "\n",
      "LOC Analysis:\n",
      "  Total unique locs: 1150\n",
      "  Total loc mentions: 1662\n",
      "  Most common locs (top 10):\n",
      "    Ä polisi: 36 times\n",
      "    Ä Polsek: 15 times\n",
      "    Ä Jalan Ä AH Ä Nasution: 11 times\n",
      "    Ä sawit: 8 times\n",
      "    Ä dua: 7 times\n",
      "    Ä Polresta: 7 times\n",
      "    Ä Jalan Ä B yp ass Ä Ngurah Ä Rai: 7 times\n",
      "    Ä Jalan Ä Jamin Ä Ginting: 7 times\n",
      "    Ä Kodam Ä I: 7 times\n",
      "    Ä Barisan: 7 times\n",
      "\n",
      "NOR Analysis:\n",
      "  Total unique nors: 391\n",
      "  Total nor mentions: 861\n",
      "  Most common nors (top 10):\n",
      "    Ä detik Sumut: 98 times\n",
      "    Ä detik Bali: 42 times\n",
      "    Ä detik S umb ag sel: 34 times\n",
      "    Ä detik Jatim: 32 times\n",
      "    Ä detik Jabar: 30 times\n",
      "    Ä detik News: 26 times\n",
      "    Ä detik Sulsel: 22 times\n",
      "    Ä fakultas Ä teknik: 16 times\n",
      "    Ä tim: 14 times\n",
      "    Ä fakultas Ä hukum: 12 times\n",
      "\n",
      "LAW Analysis:\n",
      "  Total unique laws: 297\n",
      "  Total law mentions: 452\n",
      "  Most common laws (top 10):\n",
      "    Ä penjara: 24 times\n",
      "    9: 14 times\n",
      "    15: 9 times\n",
      "    Ä pasal: 9 times\n",
      "    10: 7 times\n",
      "    5: 6 times\n",
      "    Ä Pasal Ä  365 Ä KUHP: 6 times\n",
      "    7: 6 times\n",
      "    Ä Pasal Ä  365 Ä KUHP Ä tentang: 6 times\n",
      "    Ä buku: 6 times\n",
      "\n",
      "DAT Analysis:\n",
      "  Total unique dats: 578\n",
      "  Total dat mentions: 3495\n",
      "  Most common dats (top 10):\n",
      "    Senin: 254 times\n",
      "    Kamis: 245 times\n",
      "    Rabu: 234 times\n",
      "    Selasa: 226 times\n",
      "    Jumat: 214 times\n",
      "    Sabtu: 183 times\n",
      "    Minggu: 172 times\n",
      "    malam: 150 times\n",
      "    kemarin: 55 times\n",
      "    pagi: 53 times\n",
      "\n",
      "PER Analysis:\n",
      "  Total unique pers: 2558\n",
      "  Total per mentions: 7270\n",
      "  Most common pers (top 10):\n",
      "    Ä G id ion: 92 times\n",
      "    Ä Agus: 89 times\n",
      "    Ä AS: 53 times\n",
      "    Ä R: 51 times\n",
      "    Ä A: 48 times\n",
      "    Ä Kombes Ä G id ion Ä Arif Ä Setya wan: 44 times\n",
      "    Ä AP: 41 times\n",
      "    Ä S: 35 times\n",
      "    Ä FA: 30 times\n",
      "    Ä F: 28 times\n",
      "\n",
      "CRIMETYPE Analysis:\n",
      "  Total unique crimetypes: 46\n",
      "  Total crimetype mentions: 2556\n",
      "  Most common crimetypes (top 10):\n",
      "    pencurian: 574 times\n",
      "    narkoba: 484 times\n",
      "    pembunuhan: 204 times\n",
      "    tawuran: 123 times\n",
      "    penganiayaan: 123 times\n",
      "    begal: 110 times\n",
      "    pemerkosaan: 107 times\n",
      "    penipuan: 90 times\n",
      "    curanmor: 85 times\n",
      "    perampokan: 62 times\n",
      "\n",
      "EVIDENCE Analysis:\n",
      "  Total unique evidences: 36\n",
      "  Total evidence mentions: 927\n",
      "  Most common evidences (top 10):\n",
      "    barang bukti: 229 times\n",
      "    handphone: 96 times\n",
      "    cctv: 76 times\n",
      "    pengakuan: 74 times\n",
      "    darah: 65 times\n",
      "    bukti: 44 times\n",
      "    surat: 43 times\n",
      "    laptop: 34 times\n",
      "    rekaman cctv: 32 times\n",
      "    foto: 31 times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_specific_enhanced_entities(output_df):\n",
    "\n",
    "    print(\"=== DETAILED ENHANCED ENTITY ANALYSIS ===\\n\")\n",
    "    \n",
    "    target_entities = ['gpe', 'loc', 'nor', 'law', 'dat', 'per', 'crimetype', 'evidence']\n",
    "    \n",
    "    for entity_type in target_entities:\n",
    "        entities_col = f'{entity_type}_entities'\n",
    "        \n",
    "        if entities_col in output_df.columns:\n",
    "            print(f\"{entity_type.upper()} Analysis:\")\n",
    "            \n",
    "            # Collect all entities\n",
    "            all_entities = []\n",
    "            for entities_str in output_df[entities_col].dropna():\n",
    "                if entities_str and entities_str.strip() != '':\n",
    "                    entities = [e.strip() for e in entities_str.split(';') if e.strip()]\n",
    "                    all_entities.extend(entities)\n",
    "            \n",
    "            if all_entities:\n",
    "                entity_freq = Counter(all_entities)\n",
    "                \n",
    "                print(f\"  Total unique {entity_type}s: {len(entity_freq)}\")\n",
    "                print(f\"  Total {entity_type} mentions: {len(all_entities)}\")\n",
    "                print(f\"  Most common {entity_type}s (top 10):\")\n",
    "                \n",
    "                for entity, freq in entity_freq.most_common(10):\n",
    "                    print(f\"    {entity}: {freq} times\")\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"  No {entity_type}s detected\")\n",
    "                print()\n",
    "\n",
    "# Analyze specific entities\n",
    "if 'enhanced_output' in locals():\n",
    "    analyze_specific_enhanced_entities(enhanced_output)\n",
    "else:\n",
    "    analyze_specific_enhanced_entities(sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL ENHANCED ENTITY EXTRACTION REPORT ===\n",
      "\n",
      "OVERALL STATISTICS:\n",
      "  Total articles processed: 636\n",
      "  Total entities detected: 19619\n",
      "  Articles with entities: 636 (100.0%)\n",
      "  Average entities per article: 30.85\n",
      "\n",
      "ENTITY TYPE BREAKDOWN:\n",
      "\n",
      "GPE - Geopolitical Entities (countries, cities, states)\n",
      "  Total detected: 2396\n",
      "  Article coverage: 616/636 (96.9%)\n",
      "  Average per article: 3.77\n",
      "\n",
      "LOC - Locations (places, addresses, geographical locations)\n",
      "  Total detected: 1662\n",
      "  Article coverage: 540/636 (84.9%)\n",
      "  Average per article: 2.61\n",
      "\n",
      "NOR - Organizations (companies, institutions, groups)\n",
      "  Total detected: 861\n",
      "  Article coverage: 353/636 (55.5%)\n",
      "  Average per article: 1.35\n",
      "\n",
      "LAW - Legal entities (laws, regulations, legal terms)\n",
      "  Total detected: 452\n",
      "  Article coverage: 201/636 (31.6%)\n",
      "  Average per article: 0.71\n",
      "\n",
      "DAT - Dates and times (temporal expressions)\n",
      "  Total detected: 3495\n",
      "  Article coverage: 633/636 (99.5%)\n",
      "  Average per article: 5.50\n",
      "\n",
      "PER - Persons (names of people)\n",
      "  Total detected: 7270\n",
      "  Article coverage: 634/636 (99.7%)\n",
      "  Average per article: 11.43\n",
      "\n",
      "CRIMETYPE - Crime types (custom detected criminal activities)\n",
      "  Total detected: 2556\n",
      "  Article coverage: 542/636 (85.2%)\n",
      "  Average per article: 4.02\n",
      "\n",
      "EVIDENCE - Evidence types (custom detected evidence items)\n",
      "  Total detected: 927\n",
      "  Article coverage: 373/636 (58.6%)\n",
      "  Average per article: 1.46\n",
      "\n",
      "HIGH DETECTION RATE ENTITIES (>50% coverage):\n",
      "  GPE: 96.9% coverage\n",
      "  LOC: 84.9% coverage\n",
      "  NOR: 55.5% coverage\n",
      "  DAT: 99.5% coverage\n",
      "  PER: 99.7% coverage\n",
      "  CRIMETYPE: 85.2% coverage\n",
      "  EVIDENCE: 58.6% coverage\n",
      "\n",
      "============================================================\n",
      "Enhanced entity extraction completed successfully!\n",
      "Files saved: all_enhanced_entity_results.json and all_enhanced_entity_results.csv\n"
     ]
    }
   ],
   "source": [
    "def create_final_entity_report(output_df):\n",
    "\n",
    "    print(\"=== FINAL ENHANCED ENTITY EXTRACTION REPORT ===\\n\")\n",
    "    \n",
    "    target_entities = ['gpe', 'loc', 'nor', 'law', 'dat', 'per', 'crimetype', 'evidence']\n",
    "    total_articles = len(output_df)\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_entities_detected = sum([output_df[f'{entity}_count'].sum() for entity in target_entities if f'{entity}_count' in output_df.columns])\n",
    "    articles_with_any_entity = len(output_df[output_df[[f'{entity}_count' for entity in target_entities if f'{entity}_count' in output_df.columns]].sum(axis=1) > 0])\n",
    "    \n",
    "    print(f\"OVERALL STATISTICS:\")\n",
    "    print(f\"  Total articles processed: {total_articles}\")\n",
    "    print(f\"  Total entities detected: {total_entities_detected}\")\n",
    "    print(f\"  Articles with entities: {articles_with_any_entity} ({articles_with_any_entity/total_articles*100:.1f}%)\")\n",
    "    print(f\"  Average entities per article: {total_entities_detected/total_articles:.2f}\")\n",
    "    print()\n",
    "    \n",
    "    # Detailed breakdown\n",
    "    entity_descriptions = {\n",
    "        'gpe': 'Geopolitical Entities (countries, cities, states)',\n",
    "        'loc': 'Locations (places, addresses, geographical locations)',\n",
    "        'nor': 'Organizations (companies, institutions, groups)',\n",
    "        'law': 'Legal entities (laws, regulations, legal terms)',\n",
    "        'dat': 'Dates and times (temporal expressions)',\n",
    "        'per': 'Persons (names of people)',\n",
    "        'crimetype': 'Crime types (custom detected criminal activities)',\n",
    "        'evidence': 'Evidence types (custom detected evidence items)'\n",
    "    }\n",
    "    \n",
    "    print(\"ENTITY TYPE BREAKDOWN:\")\n",
    "    for entity_type in target_entities:\n",
    "        count_col = f'{entity_type}_count'\n",
    "        if count_col in output_df.columns:\n",
    "            total = output_df[count_col].sum()\n",
    "            coverage = len(output_df[output_df[count_col] > 0])\n",
    "            percentage = (coverage / total_articles) * 100\n",
    "            \n",
    "            print(f\"\\n{entity_type.upper()} - {entity_descriptions[entity_type]}\")\n",
    "            print(f\"  Total detected: {total}\")\n",
    "            print(f\"  Article coverage: {coverage}/{total_articles} ({percentage:.1f}%)\")\n",
    "            print(f\"  Average per article: {total/total_articles:.2f}\")\n",
    "    \n",
    "    # Success metrics\n",
    "    highly_detected_entities = [entity for entity in target_entities \n",
    "                              if f'{entity}_count' in output_df.columns and \n",
    "                              len(output_df[output_df[f'{entity}_count'] > 0]) / total_articles > 0.5]\n",
    "    \n",
    "    print(f\"\\nHIGH DETECTION RATE ENTITIES (>50% coverage):\")\n",
    "    for entity in highly_detected_entities:\n",
    "        coverage = len(output_df[output_df[f'{entity}_count'] > 0]) / total_articles * 100\n",
    "        print(f\"  {entity.upper()}: {coverage:.1f}% coverage\")\n",
    "    \n",
    "    return {\n",
    "        'total_articles': total_articles,\n",
    "        'total_entities': total_entities_detected,\n",
    "        'articles_with_entities': articles_with_any_entity,\n",
    "        'high_coverage_entities': highly_detected_entities\n",
    "    }\n",
    "\n",
    "# Generate final report\n",
    "if 'enhanced_output' in locals():\n",
    "    final_report = create_final_entity_report(enhanced_output)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Enhanced entity extraction completed successfully!\")\n",
    "    print(\"Files saved: all_enhanced_entity_results.json and all_enhanced_entity_results.csv\")\n",
    "else:\n",
    "    final_report = create_final_entity_report(sample_output)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Sample enhanced entity extraction completed!\")\n",
    "    print(\"Files saved: sample_enhanced_results.json and sample_enhanced_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dates_pattern(text):\n",
    "\n",
    "    import re\n",
    "    detected_dates = []\n",
    "    \n",
    "    # Pattern untuk berbagai format tanggal Indonesia\n",
    "    date_patterns = [\n",
    "        r'\\b\\d{1,2}[\\s\\-/]\\w+[\\s\\-/]\\d{4}\\b',  # 15 Januari 2024, 15-01-2024\n",
    "        r'\\b\\d{1,2}[\\s\\-/]\\d{1,2}[\\s\\-/]\\d{4}\\b',  # 15/01/2024, 15-01-2024\n",
    "        r'\\b\\d{4}[\\s\\-/]\\d{1,2}[\\s\\-/]\\d{1,2}\\b',  # 2024-01-15\n",
    "        r'\\b(?:senin|selasa|rabu|kamis|jumat|sabtu|minggu)\\b',  # Hari\n",
    "        r'\\b(?:januari|februari|maret|april|mei|juni|juli|agustus|september|oktober|november|desember)\\b',  # Bulan\n",
    "        r'\\b\\d{1,2}\\s*(?:januari|februari|maret|april|mei|juni|juli|agustus|september|oktober|november|desember)\\s*\\d{4}\\b',  # 15 Januari 2024\n",
    "        r'\\bpukul\\s*\\d{1,2}[:.]\\d{2}\\b',  # pukul 14:30\n",
    "        r'\\b\\d{1,2}[:.]\\d{2}\\s*(?:WIB|WITA|WIT)\\b',  # 14:30 WIB\n",
    "        r'\\b(?:kemarin|hari\\s*ini|besok|lusa)\\b',  # Referensi waktu relatif\n",
    "        r'\\b(?:pagi|siang|sore|malam)\\s*(?:hari|ini|kemarin|tadi)?\\b'  # Waktu dalam hari\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for pattern in date_patterns:\n",
    "        matches = re.finditer(pattern, text_lower, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            detected_dates.append({\n",
    "                'entity': 'DAT',\n",
    "                'word': text[match.start():match.end()],\n",
    "                'start': match.start(),\n",
    "                'end': match.end(),\n",
    "                'score': 0.95\n",
    "            })\n",
    "    \n",
    "    return detected_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7552854,
     "sourceId": 12005979,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
